{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PN0ke9lyoGJJ"
      },
      "source": [
        "# Setting up Reddit  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TgaysFMSoGJN"
      },
      "source": [
        "\n",
        "Using the PRAW library, a wrapper for the Reddit API, everyone can easily scrape data from Reddit or even create a Reddit bot."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X3u8q9XDoGJN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1cad5459-60a8-40e5-bc80-c0979c913891"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting praw\n",
            "  Downloading praw-7.7.1-py3-none-any.whl (191 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m191.0/191.0 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting prawcore<3,>=2.1 (from praw)\n",
            "  Downloading prawcore-2.4.0-py3-none-any.whl (17 kB)\n",
            "Collecting update-checker>=0.18 (from praw)\n",
            "  Downloading update_checker-0.18.0-py3-none-any.whl (7.0 kB)\n",
            "Requirement already satisfied: websocket-client>=0.54.0 in /usr/local/lib/python3.10/dist-packages (from praw) (1.7.0)\n",
            "Requirement already satisfied: requests<3.0,>=2.6.0 in /usr/local/lib/python3.10/dist-packages (from prawcore<3,>=2.1->praw) (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.1->praw) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.1->praw) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.1->praw) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.1->praw) (2024.2.2)\n",
            "Installing collected packages: update-checker, prawcore, praw\n",
            "Successfully installed praw-7.7.1 prawcore-2.4.0 update-checker-0.18.0\n",
            "Collecting asyncpraw\n",
            "  Downloading asyncpraw-7.7.1-py3-none-any.whl (196 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.7/196.7 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting aiofiles<1 (from asyncpraw)\n",
            "  Downloading aiofiles-0.8.0-py3-none-any.whl (13 kB)\n",
            "Requirement already satisfied: aiohttp<4 in /usr/local/lib/python3.10/dist-packages (from asyncpraw) (3.9.3)\n",
            "Collecting aiosqlite<=0.17.0 (from asyncpraw)\n",
            "  Downloading aiosqlite-0.17.0-py3-none-any.whl (15 kB)\n",
            "Collecting asyncprawcore<3,>=2.1 (from asyncpraw)\n",
            "  Downloading asyncprawcore-2.4.0-py3-none-any.whl (19 kB)\n",
            "Requirement already satisfied: update-checker>=0.18 in /usr/local/lib/python3.10/dist-packages (from asyncpraw) (0.18.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4->asyncpraw) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4->asyncpraw) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4->asyncpraw) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4->asyncpraw) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4->asyncpraw) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4->asyncpraw) (4.0.3)\n",
            "Requirement already satisfied: typing_extensions>=3.7.2 in /usr/local/lib/python3.10/dist-packages (from aiosqlite<=0.17.0->asyncpraw) (4.9.0)\n",
            "Requirement already satisfied: requests>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from update-checker>=0.18->asyncpraw) (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.3.0->update-checker>=0.18->asyncpraw) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.3.0->update-checker>=0.18->asyncpraw) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.3.0->update-checker>=0.18->asyncpraw) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.3.0->update-checker>=0.18->asyncpraw) (2024.2.2)\n",
            "Installing collected packages: aiosqlite, aiofiles, asyncprawcore, asyncpraw\n",
            "Successfully installed aiofiles-0.8.0 aiosqlite-0.17.0 asyncpraw-7.7.1 asyncprawcore-2.4.0\n"
          ]
        }
      ],
      "source": [
        "!pip install praw\n",
        "!pip install --upgrade asyncpraw\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UumMBoWLoGJP"
      },
      "outputs": [],
      "source": [
        "import praw\n",
        "import asyncpraw\n",
        "from aiohttp import ClientSession"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aV4bHrBMoGJQ"
      },
      "outputs": [],
      "source": [
        "reddit = praw.Reddit(client_id='PJxseFpHblIKsIPoDmWwyQ',\n",
        "                     client_secret='En8dmoO9CLURm8mybarj9d2wDNl7EA',\n",
        "                     user_agent='testing the apo end points')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Parsing date from Reddit API\n",
        "\n",
        "\n",
        "*   Ouput text file from date of posting\n",
        "*   Sorting posts on the number of upvotes\n",
        "\n"
      ],
      "metadata": {
        "id": "Er-vsFD9SI0m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import datetime\n",
        "\n",
        "# Define the flairs you're interested in\n",
        "flairs = [\"Investieren - Aktien\", \"Investieren - ETF\"]\n",
        "\n",
        "# Get all submissions in the subreddit\n",
        "submissions = []\n",
        "for submission in reddit.subreddit('Finanzen').hot(limit=2):\n",
        "    # Check if the post's flair is in the list of flairs\n",
        "    if submission.link_flair_text in flairs:\n",
        "        # Calculate the post's creation date\n",
        "        created_utc = submission.created_utc\n",
        "        post_created = datetime.datetime.fromtimestamp(created_utc)\n",
        "        post_created = post_created.strftime(\"%Y%m%d\")\n",
        "\n",
        "        # Add the post and its creation date to the submissions list\n",
        "        submissions.append((submission, post_created))\n",
        "\n",
        "# Sort the submissions by their creation date in descending order\n",
        "sorted_submissions = sorted(submissions, key=lambda s: s[1], reverse=True)\n",
        "title_list =[]\n",
        "comments_list = []\n",
        "# Process each submission\n",
        "for i, (submission, post_created) in enumerate(sorted_submissions, start=1):\n",
        "    # Your existing processing code here...\n",
        "    # Print the title, selftext, and url\n",
        "    title = 'QUESTION- ' + submission.title\n",
        "    title_list.append(title)\n",
        "    titletext = 'QUESTION DESCRIPTION- ' + submission.selftext\n",
        "    titleurl = 'QUESTION REFERENCE- ' + submission.url\n",
        "    score = submission.score\n",
        "    Popularity = 'NUMBER OF VOTES- ' + str(score)\n",
        "    post = 'Date- ' + post_created\n",
        "    # Add the LLM summarizer here (maybe mistral)\n",
        "\n",
        "    # Replace \"MoreComments\" objects with actual Comment objects\n",
        "    submission.comments.replace_more(limit=None)\n",
        "    if (score >90):\n",
        "        # Output to a text file\n",
        "\n",
        "        #title_list.append(str(title))\n",
        "        with open(f'{post}_{score}.txt', 'w') as file:\n",
        "            file.write(title + '\\n\\n')\n",
        "            file.write(titletext + '\\n\\n')\n",
        "            #file.write(submission.selftext + '\\n\\n')\n",
        "            file.write(titleurl + '\\n\\n')\n",
        "            file.write(post +'\\n\\n')\n",
        "            file.write(Popularity+ '\\n\\n')\n",
        "\n",
        "            # Initialize a counter for the discussions\n",
        "            discussion_counter = 1\n",
        "\n",
        "            # Iterate over the comments\n",
        "            for comment in submission.comments.list():\n",
        "                # Remove extra spaces from the comment\n",
        "                clean_comment = comment.body.replace(' ', ' ')\n",
        "                # Write the discussion number and comment to the file, starting on a new line\n",
        "                file.write(f'\\n\\nDiscussion {discussion_counter}:\\n\\n{clean_comment}\\n\\n')\n",
        "                discussion_counter += 1\n",
        "                comments_list.append(clean_comment)"
      ],
      "metadata": {
        "id": "yLdeb2boV84v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(comments_list)\n",
        "print(type(comments_list))"
      ],
      "metadata": {
        "id": "5fhNcJXh0TNw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Using Json as output instead"
      ],
      "metadata": {
        "id": "pqY8LDMC7wab"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# MetaData\n",
        "import praw\n",
        "import datetime\n",
        "import json\n",
        "\n",
        "# Define the flairs you're interested in\n",
        "flairs = [\"Investieren - Aktien\", \"Investieren - ETF\"]\n",
        "\n",
        "# Get all submissions in the subreddit\n",
        "submissions = []\n",
        "for submission in reddit.subreddit('Finanzen').hot(limit=None):\n",
        "    # Check if the post's flair is in the list of flairs\n",
        "    if submission.link_flair_text in flairs:\n",
        "        # Calculate the post's creation date\n",
        "        created_utc = submission.created_utc\n",
        "        post_created = datetime.datetime.fromtimestamp(created_utc)\n",
        "        post_created = post_created.strftime(\"%Y%m%d\")\n",
        "\n",
        "        # Add the post and its creation date to the submissions list\n",
        "        submissions.append((submission, post_created))\n",
        "\n",
        "# Sort the submissions by their creation date in descending order\n",
        "sorted_submissions = sorted(submissions, key=lambda s: s[1], reverse=True)\n",
        "\n",
        "# Process each submission and add it to a list of submission dictionaries\n",
        "submission_list = []\n",
        "for i, (submission, post_created) in enumerate(sorted_submissions, start=1):\n",
        "    # Your existing processing code here...\n",
        "    # Print the title, selftext, and url\n",
        "    title =  submission.title\n",
        "    titletext = submission.selftext\n",
        "    titleurl =  submission.url\n",
        "    score = submission.score\n",
        "    Popularity = score\n",
        "    post = post_created\n",
        "\n",
        "    # Replace \"MoreComments\" objects with actual Comment objects\n",
        "    submission.comments.replace_more(limit=None)\n",
        "    # Create a dictionary with the submission details, with metadata nesting\n",
        "    submission_info = {\n",
        "            'title': title,\n",
        "            'description': titletext,\n",
        "            'metadata': {\n",
        "                'reference': titleurl,\n",
        "                'date': post,\n",
        "                'popularity': Popularity\n",
        "            },\n",
        "            'comments': [{'number': i+1, 'content': comment.body} for i, comment in enumerate(submission.comments.list())]\n",
        "        }\n",
        "\n",
        "    # Add the submission_info dictionary to the submission_list\n",
        "    submission_list.append(submission_info)\n",
        "\n",
        "# Write the submission_list to a single JSON file\n",
        "with open(\"submissionsmetadata.json\", 'w') as json_file:\n",
        "    json.dump(submission_list, json_file, indent=4)\n"
      ],
      "metadata": {
        "id": "S2RzRlgZ5EHt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls\n",
        "!rm *.json"
      ],
      "metadata": {
        "id": "x8mM735s6osg",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Multiple json files\n",
        "import praw\n",
        "import datetime\n",
        "import json\n",
        "\n",
        "# Define the flairs you're interested in\n",
        "flairs = [\"Investieren - Aktien\", \"Investieren - ETF\"]\n",
        "\n",
        "# Get all submissions in the subreddit\n",
        "submissions = []\n",
        "for submission in reddit.subreddit('Finanzen').hot(limit=None):\n",
        "    # Check if the post's flair is in the list of flairs\n",
        "    if submission.link_flair_text in flairs:\n",
        "        # Calculate the post's creation date\n",
        "        created_utc = submission.created_utc\n",
        "        post_created = datetime.datetime.fromtimestamp(created_utc)\n",
        "        post_created = post_created.strftime(\"%Y%m%d\")\n",
        "\n",
        "        # Add the post and its creation date to the submissions list\n",
        "        submissions.append((submission, post_created))\n",
        "\n",
        "# Sort the submissions by their creation date in descending order\n",
        "sorted_submissions = sorted(submissions, key=lambda s: s[1], reverse=True)\n",
        "\n",
        "# Process each submission\n",
        "for i, (submission, post_created) in enumerate(sorted_submissions, start=1):\n",
        "    # Your existing processing code here...\n",
        "    # Print the title, selftext, and url\n",
        "    title =  submission.title\n",
        "    titletext = submission.selftext\n",
        "    titleurl =  submission.url\n",
        "    score = submission.score\n",
        "    Popularity = score\n",
        "    post = post_created\n",
        "\n",
        "    # Replace \"MoreComments\" objects with actual Comment objects\n",
        "    submission.comments.replace_more(limit=None)\n",
        "    #if (score >   90):\n",
        "        # Create a dictionary with the submission details\n",
        "    submission_info = {\n",
        "            'title': title,\n",
        "            'description': titletext,\n",
        "            'reference': titleurl,\n",
        "            'date': post,\n",
        "            'popularity': Popularity,\n",
        "            'comments': [{'number': i+1, 'content': comment.body} for i, comment in enumerate(submission.comments.list())]\n",
        "        }\n",
        "\n",
        "    # Generate a unique filename for the JSON file using the index of the loop\n",
        "    filename = f\"submission_{i}.json\"\n",
        "\n",
        "    # Convert the dictionary to a JSON string\n",
        "    json_output = json.dumps(submission_info, indent=4)\n",
        "\n",
        "    # Write the JSON string to a file\n",
        "    with open(filename, 'w') as json_file:\n",
        "        json_file.write(json_output)\n",
        "\n"
      ],
      "metadata": {
        "id": "Y-WAp1qV72G-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SingleJson File"
      ],
      "metadata": {
        "id": "GI3TZx7Wucni"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import praw\n",
        "import datetime\n",
        "import json\n",
        "\n",
        "# Define the flairs you're interested in\n",
        "flairs = [\"Investieren - Aktien\", \"Investieren - ETF\"]\n",
        "\n",
        "# Get all submissions in the subreddit\n",
        "submissions = []\n",
        "for submission in reddit.subreddit('Finanzen').hot(limit=None):\n",
        "    # Check if the post's flair is in the list of flairs\n",
        "    if submission.link_flair_text in flairs:\n",
        "        # Calculate the post's creation date\n",
        "        created_utc = submission.created_utc\n",
        "        post_created = datetime.datetime.fromtimestamp(created_utc)\n",
        "        post_created = post_created.strftime(\"%Y%m%d\")\n",
        "\n",
        "        # Add the post and its creation date to the submissions list\n",
        "        submissions.append((submission, post_created))\n",
        "\n",
        "# Sort the submissions by their creation date in descending order\n",
        "sorted_submissions = sorted(submissions, key=lambda s: s[1], reverse=True)\n",
        "\n",
        "# Process each submission and add it to a list of submission dictionaries\n",
        "submission_list = []\n",
        "for i, (submission, post_created) in enumerate(sorted_submissions, start=1):\n",
        "    # Your existing processing code here...\n",
        "    # Print the title, selftext, and url\n",
        "    title =  submission.title\n",
        "    titletext = submission.selftext\n",
        "    titleurl =  submission.url\n",
        "    score = submission.score\n",
        "    Popularity = score\n",
        "    post = post_created\n",
        "\n",
        "    # Replace \"MoreComments\" objects with actual Comment objects\n",
        "    submission.comments.replace_more(limit=None)\n",
        "    # Create a dictionary with the submission details\n",
        "    submission_info = {\n",
        "            'title': title,\n",
        "            'description': titletext,\n",
        "            'reference': titleurl,\n",
        "            'date': post,\n",
        "            'popularity': Popularity,\n",
        "            'comments': [{'number': i+1, 'content': comment.body} for i, comment in enumerate(submission.comments.list())]\n",
        "        }\n",
        "\n",
        "    # Add the submission_info dictionary to the submission_list\n",
        "    submission_list.append(submission_info)\n",
        "\n",
        "# Write the submission_list to a single JSON file\n",
        "with open(\"submissions.json\", 'w') as json_file:\n",
        "    json.dump(submission_list, json_file, indent=4)\n"
      ],
      "metadata": {
        "id": "QTLxzcQCufSU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Write the Json Schema to validate the output structure .\n",
        "\n",
        "\n",
        "*   This is needed to feed in the data to json loader\n",
        "*   Json loader + document parsing would be helpful to further process\n",
        "\n"
      ],
      "metadata": {
        "id": "MiKZ5pb5y6J9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Jsonschema\n",
        "\n",
        "schema = {\n",
        "  \"$schema\": \"http://json-schema.org/draft-07/schema#\",\n",
        "  \"type\": \"array\",\n",
        "  \"items\": {\n",
        "    \"type\": \"object\",\n",
        "    \"properties\": {\n",
        "      \"title\": {\n",
        "        \"type\": \"string\"\n",
        "      },\n",
        "      \"description\": {\n",
        "        \"type\": \"string\"\n",
        "      },\n",
        "      \"metadata\": {\n",
        "        \"type\": \"object\",\n",
        "        \"properties\": {\n",
        "          \"reference\": {\n",
        "            \"type\": \"string\",\n",
        "            \"format\": \"uri\"\n",
        "          },\n",
        "          \"date\": {\n",
        "            \"type\": \"string\",\n",
        "            \"pattern\": \"^\\\\d{8}$\"\n",
        "          },\n",
        "          \"popularity\": {\n",
        "            \"type\": \"integer\",\n",
        "            \"minimum\":  0\n",
        "          }\n",
        "        },\n",
        "        \"required\": [\"reference\", \"date\", \"popularity\"]\n",
        "      },\n",
        "      \"comments\": {\n",
        "        \"type\": \"array\",\n",
        "        \"items\": {\n",
        "          \"type\": \"object\",\n",
        "          \"properties\": {\n",
        "            \"number\": {\n",
        "              \"type\": \"integer\",\n",
        "              \"minimum\":  1\n",
        "            },\n",
        "            \"content\": {\n",
        "              \"type\": \"string\"\n",
        "            }\n",
        "          },\n",
        "          \"required\": [\"number\", \"content\"]\n",
        "        }\n",
        "      }\n",
        "    },\n",
        "    \"required\": [\"title\", \"description\", \"metadata\", \"comments\"]\n",
        "  }\n",
        "}\n"
      ],
      "metadata": {
        "id": "wMQuuUOs2AEA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install jsonschema"
      ],
      "metadata": {
        "id": "GX8Dn_1SzMKN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from jsonschema import validate, ValidationError\n",
        "\n",
        "# Load the JSON schema\n",
        "with open('/content/schema.json', 'r') as schema_file:\n",
        "    schema = json.load(schema_file)\n",
        "\n",
        "# Load the JSON data to be validated\n",
        "with open('/content/submissionsmetadata.json', 'r') as data_file:\n",
        "    data = json.load(data_file)\n",
        "\n",
        "# Validate the data against the schema\n",
        "try:\n",
        "    validate(instance=data, schema=schema)\n",
        "    print(\"JSON data is valid.\")\n",
        "except ValidationError as e:\n",
        "    print(f\"JSON data is invalid. Error: {e.message}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XZDLXd0g0sK4",
        "outputId": "00e174c2-38e3-40e3-ace0-f980d6280d6f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "JSON data is valid.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Advanced RAG systems langchain (hugging face cookbook)"
      ],
      "metadata": {
        "id": "PFGAhbuQ6M7v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain"
      ],
      "metadata": {
        "id": "K0HCtYQ8AAE2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q torch transformers transformers accelerate bitsandbytes langchain sentence-transformers faiss-gpu openpyxl pacmap"
      ],
      "metadata": {
        "id": "hThFt-q46T2k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.docstore.document import Document as LangchainDocument\n",
        "from langchain_community.document_loaders import JSONLoader"
      ],
      "metadata": {
        "id": "41dpHRLP6jXV"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "PN0ke9lyoGJJ"
      ],
      "gpuType": "T4",
      "toc_visible": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}